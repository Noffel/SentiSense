{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vector = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import itertools\n",
    "import sys\n",
    "from pandas import DataFrame\n",
    "from keras.models import Sequential\n",
    "\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Activation\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "from nltk.stem import PorterStemmer\n",
    "from textblob import Word\n",
    "\n",
    "# set the matplotlib backend so figures can be saved in the background\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    " \n",
    "# import the necessary packages\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from pyimagesearch.smallervggnet import SmallerVGGNet\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#from imutils import paths\n",
    "import argparse\n",
    "import random\n",
    "import pickle\n",
    "import cv2\n",
    "\n",
    "from bs4 import BeautifulSoup \n",
    "import re # For regular expressions\n",
    "\n",
    "\n",
    "# Fitting a random forest classifier to the training data\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "#For Tokenizing and pad sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "\n",
    "from socket import *\n",
    "import socket\n",
    "from _thread import *\n",
    "import threading \n",
    "import json\n",
    "from io import StringIO\n",
    "\n",
    "\n",
    "\n",
    "import string\n",
    "\n",
    "#For Lemmatization\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "",
    "_uuid": ""
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "text_emotion = pd.read_csv(\"text_emotion.csv\")\n",
    "text_emotion=text_emotion[text_emotion.sentiment != 'enthusiasm']\n",
    "text_emotion=text_emotion[text_emotion.sentiment != 'neutral']\n",
    "text_emotion=text_emotion[text_emotion.sentiment != 'empty']\n",
    "text_emotion=text_emotion[text_emotion.sentiment != 'surprise']\n",
    "text_emotion=text_emotion[text_emotion.sentiment != 'boredom']\n",
    "text_emotion=text_emotion[text_emotion.sentiment != 'relief']\n",
    "text_emotion=text_emotion[text_emotion.sentiment != 'worry']\n",
    "text_emotion=text_emotion[text_emotion.sentiment != 'love']\n",
    "text_emotion=text_emotion[text_emotion.sentiment != 'fun']\n",
    "text_emotion['sentiment'] = text_emotion['sentiment'].map({'sadness': \"sadness\",'hate':'anger',\n",
    "                                                           'happiness': \"joy\",\n",
    "                                                           'anger': \"anger\"})\n",
    "text_emotion=text_emotion.drop(columns=['tweet_id', 'author'])\n",
    "text_emotion = text_emotion.reindex(sorted(text_emotion.columns), axis=1)\n",
    "text_emotion.columns = ['content', 'Affect']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "",
    "_uuid": ""
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>content</th>\n",
       "      <th>Affect</th>\n",
       "      <th>Intensity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-En-40427</td>\n",
       "      <td>@VivYau is it all doom and gloom? I only want ...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-En-21180</td>\n",
       "      <td>The 2nd step to beating #anxiety or #depressio...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-En-10175</td>\n",
       "      <td>Forgot to eat dinner and now I'm furious with ...</td>\n",
       "      <td>anger</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-En-31406</td>\n",
       "      <td>Your future is bright. #Remember</td>\n",
       "      <td>joy</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-En-30487</td>\n",
       "      <td>Check out this #film Robocoq 301 #animated #sh...</td>\n",
       "      <td>joy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2017-En-40386</td>\n",
       "      <td>@jccrocker #CharlotteProtest do u #wait 4 the ...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2017-En-11234</td>\n",
       "      <td>@russbully Ended up paying 75p for half a tube...</td>\n",
       "      <td>anger</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2017-En-21999</td>\n",
       "      <td>Tho we haven't talked Jeff but the news is so ...</td>\n",
       "      <td>fear</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2017-En-10288</td>\n",
       "      <td>luv seeing a man with a scowl on his face walk...</td>\n",
       "      <td>anger</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2017-En-20033</td>\n",
       "      <td>Just want Saturday to be over but then again I...</td>\n",
       "      <td>fear</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2017-En-21493</td>\n",
       "      <td>I love my mother, but talking about the recent...</td>\n",
       "      <td>fear</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2017-En-31598</td>\n",
       "      <td>Will WHU be old bill free by the time the game...</td>\n",
       "      <td>joy</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2017-En-30005</td>\n",
       "      <td>Today I reached 1000 subscribers on YT!! , #go...</td>\n",
       "      <td>joy</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2017-En-40824</td>\n",
       "      <td>On bedrest since I got out of the hospital. U ...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2017-En-10312</td>\n",
       "      <td>@Idubbbz @LeafyIsHere  I am offended</td>\n",
       "      <td>anger</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2017-En-41256</td>\n",
       "      <td>@MissFushiGaming I hashtag things and the kids...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2017-En-30293</td>\n",
       "      <td>Do what makes you successful and #happy now an...</td>\n",
       "      <td>joy</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2017-En-11458</td>\n",
       "      <td>Hey all you white people out there; are you #o...</td>\n",
       "      <td>anger</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2017-En-11614</td>\n",
       "      <td>@atheist_taco @DrJillStein :^) well cucks amon...</td>\n",
       "      <td>anger</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2017-En-10010</td>\n",
       "      <td>im so mad about power rangers. im incensed. im...</td>\n",
       "      <td>anger</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2017-En-41323</td>\n",
       "      <td>Honestly don't know why I'm so unhappy most of...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2017-En-20270</td>\n",
       "      <td>@JaySekulow what can we do 2 get @realDonaldTr...</td>\n",
       "      <td>fear</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2017-En-40446</td>\n",
       "      <td>@LazyBoiSam blues... blues? ??</td>\n",
       "      <td>sadness</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2017-En-30316</td>\n",
       "      <td>Watch this amazing live.ly broadcast by @kana_...</td>\n",
       "      <td>joy</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2017-En-20254</td>\n",
       "      <td>a panic attack AND CALL YOURSELF A REAL FAN ma...</td>\n",
       "      <td>fear</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2017-En-31392</td>\n",
       "      <td>@WildRoverTours Thank you for follow and its a...</td>\n",
       "      <td>joy</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2017-En-40313</td>\n",
       "      <td>How do u grieve someone who legally wasn't a p...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2017-En-11200</td>\n",
       "      <td>@noris_prosk8r2 (Maybe don't provoke him in th...</td>\n",
       "      <td>anger</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2017-En-40130</td>\n",
       "      <td>When will the weeks full of Mondays end?? #dis...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2017-En-22026</td>\n",
       "      <td>With only 7 months left until I possess my und...</td>\n",
       "      <td>fear</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3996</th>\n",
       "      <td>2017-En-41009</td>\n",
       "      <td>@ChipotleTweets Gabe the worst part is I can't...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3997</th>\n",
       "      <td>2017-En-40321</td>\n",
       "      <td>@HillaryClinton @TheDemocrats It's ok 2 be dis...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3998</th>\n",
       "      <td>2017-En-10293</td>\n",
       "      <td>if we let that in id be fuming poor keeping</td>\n",
       "      <td>anger</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999</th>\n",
       "      <td>2017-En-10140</td>\n",
       "      <td>All hell is breaking loose in Charlotte. #Char...</td>\n",
       "      <td>anger</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4000</th>\n",
       "      <td>2017-En-21916</td>\n",
       "      <td>Gosh #anxiety attacks turning into #Panic atta...</td>\n",
       "      <td>fear</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4001</th>\n",
       "      <td>2017-En-20652</td>\n",
       "      <td>@PatBlanchfield so you mean like Uber but for...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4002</th>\n",
       "      <td>2017-En-21213</td>\n",
       "      <td>I don't want speak front to him   #nopanicattack</td>\n",
       "      <td>fear</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4003</th>\n",
       "      <td>2017-En-11304</td>\n",
       "      <td>Get to the gym and discover I forgot to put my...</td>\n",
       "      <td>anger</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4004</th>\n",
       "      <td>2017-En-11224</td>\n",
       "      <td>The ppl on here defending cops as they continu...</td>\n",
       "      <td>anger</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4005</th>\n",
       "      <td>2017-En-40003</td>\n",
       "      <td>My #Fibromyalgia has been really bad lately wh...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4006</th>\n",
       "      <td>2017-En-41247</td>\n",
       "      <td>oh, btw - after a 6 month depression-free time...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4007</th>\n",
       "      <td>2017-En-30113</td>\n",
       "      <td>@Matalan when the lady in Xmas dept answers yo...</td>\n",
       "      <td>joy</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4008</th>\n",
       "      <td>2017-En-40917</td>\n",
       "      <td>Candice's pout is gonna take someone eye out m...</td>\n",
       "      <td>anger</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4009</th>\n",
       "      <td>2017-En-30374</td>\n",
       "      <td>@gypsydragoness wrinkling up. 'Well now, looks...</td>\n",
       "      <td>joy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4010</th>\n",
       "      <td>2017-En-30241</td>\n",
       "      <td>What if.... the Metro LRT went over the Walter...</td>\n",
       "      <td>joy</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4011</th>\n",
       "      <td>2017-En-22137</td>\n",
       "      <td>Somewhere between  #hope and #despair the day ...</td>\n",
       "      <td>fear</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4012</th>\n",
       "      <td>2017-En-30539</td>\n",
       "      <td>Decide to stop being afraid. To just stop. Wha...</td>\n",
       "      <td>joy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4013</th>\n",
       "      <td>2017-En-11685</td>\n",
       "      <td>@robert7019 @itzamadhouse @skyler12388\\r\\ralso...</td>\n",
       "      <td>anger</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4014</th>\n",
       "      <td>2017-En-40354</td>\n",
       "      <td>@chelseafc let them know it's the #blues</td>\n",
       "      <td>sadness</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4015</th>\n",
       "      <td>2017-En-20122</td>\n",
       "      <td>Well this is flipping great! Flipping standsti...</td>\n",
       "      <td>fear</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4016</th>\n",
       "      <td>2017-En-40378</td>\n",
       "      <td>the waitress recognised me from last time i wa...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4017</th>\n",
       "      <td>2017-En-30311</td>\n",
       "      <td>Watch this amazing live.ly broadcast by @evanh...</td>\n",
       "      <td>joy</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4018</th>\n",
       "      <td>2017-En-31280</td>\n",
       "      <td>Go one extra mile just see her smiling</td>\n",
       "      <td>joy</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4019</th>\n",
       "      <td>2017-En-10383</td>\n",
       "      <td>Yet we still have deaths, road rage, &amp;amp; vio...</td>\n",
       "      <td>anger</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4020</th>\n",
       "      <td>2017-En-40289</td>\n",
       "      <td>Dylon felt dejected. He has a dejected aunt!</td>\n",
       "      <td>sadness</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4021</th>\n",
       "      <td>2017-En-40146</td>\n",
       "      <td>With loyal heart is concerned. #somber</td>\n",
       "      <td>sadness</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4022</th>\n",
       "      <td>2017-En-10119</td>\n",
       "      <td>@XemitSellsMagic add tracking but resent them</td>\n",
       "      <td>anger</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4023</th>\n",
       "      <td>2017-En-11153</td>\n",
       "      <td>Having one of those #angry days. I will have t...</td>\n",
       "      <td>anger</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4024</th>\n",
       "      <td>2017-En-11123</td>\n",
       "      <td>@daemondave @paulkrugman Hey stupid, that was ...</td>\n",
       "      <td>anger</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4025</th>\n",
       "      <td>7102</td>\n",
       "      <td>im happy</td>\n",
       "      <td>joy</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4026 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ID                                            content  \\\n",
       "0     2017-En-40427  @VivYau is it all doom and gloom? I only want ...   \n",
       "1     2017-En-21180  The 2nd step to beating #anxiety or #depressio...   \n",
       "2     2017-En-10175  Forgot to eat dinner and now I'm furious with ...   \n",
       "3     2017-En-31406                   Your future is bright. #Remember   \n",
       "4     2017-En-30487  Check out this #film Robocoq 301 #animated #sh...   \n",
       "5     2017-En-40386  @jccrocker #CharlotteProtest do u #wait 4 the ...   \n",
       "6     2017-En-11234  @russbully Ended up paying 75p for half a tube...   \n",
       "7     2017-En-21999  Tho we haven't talked Jeff but the news is so ...   \n",
       "8     2017-En-10288  luv seeing a man with a scowl on his face walk...   \n",
       "9     2017-En-20033  Just want Saturday to be over but then again I...   \n",
       "10    2017-En-21493  I love my mother, but talking about the recent...   \n",
       "11    2017-En-31598  Will WHU be old bill free by the time the game...   \n",
       "12    2017-En-30005  Today I reached 1000 subscribers on YT!! , #go...   \n",
       "13    2017-En-40824  On bedrest since I got out of the hospital. U ...   \n",
       "14    2017-En-10312               @Idubbbz @LeafyIsHere  I am offended   \n",
       "15    2017-En-41256  @MissFushiGaming I hashtag things and the kids...   \n",
       "16    2017-En-30293  Do what makes you successful and #happy now an...   \n",
       "17    2017-En-11458  Hey all you white people out there; are you #o...   \n",
       "18    2017-En-11614  @atheist_taco @DrJillStein :^) well cucks amon...   \n",
       "19    2017-En-10010  im so mad about power rangers. im incensed. im...   \n",
       "20    2017-En-41323  Honestly don't know why I'm so unhappy most of...   \n",
       "21    2017-En-20270  @JaySekulow what can we do 2 get @realDonaldTr...   \n",
       "22    2017-En-40446                     @LazyBoiSam blues... blues? ??   \n",
       "23    2017-En-30316  Watch this amazing live.ly broadcast by @kana_...   \n",
       "24    2017-En-20254  a panic attack AND CALL YOURSELF A REAL FAN ma...   \n",
       "25    2017-En-31392  @WildRoverTours Thank you for follow and its a...   \n",
       "26    2017-En-40313  How do u grieve someone who legally wasn't a p...   \n",
       "27    2017-En-11200  @noris_prosk8r2 (Maybe don't provoke him in th...   \n",
       "28    2017-En-40130  When will the weeks full of Mondays end?? #dis...   \n",
       "29    2017-En-22026  With only 7 months left until I possess my und...   \n",
       "...             ...                                                ...   \n",
       "3996  2017-En-41009  @ChipotleTweets Gabe the worst part is I can't...   \n",
       "3997  2017-En-40321  @HillaryClinton @TheDemocrats It's ok 2 be dis...   \n",
       "3998  2017-En-10293        if we let that in id be fuming poor keeping   \n",
       "3999  2017-En-10140  All hell is breaking loose in Charlotte. #Char...   \n",
       "4000  2017-En-21916  Gosh #anxiety attacks turning into #Panic atta...   \n",
       "4001  2017-En-20652  @PatBlanchfield so you mean like Uber but for...   \n",
       "4002  2017-En-21213   I don't want speak front to him   #nopanicattack   \n",
       "4003  2017-En-11304  Get to the gym and discover I forgot to put my...   \n",
       "4004  2017-En-11224  The ppl on here defending cops as they continu...   \n",
       "4005  2017-En-40003  My #Fibromyalgia has been really bad lately wh...   \n",
       "4006  2017-En-41247  oh, btw - after a 6 month depression-free time...   \n",
       "4007  2017-En-30113  @Matalan when the lady in Xmas dept answers yo...   \n",
       "4008  2017-En-40917  Candice's pout is gonna take someone eye out m...   \n",
       "4009  2017-En-30374  @gypsydragoness wrinkling up. 'Well now, looks...   \n",
       "4010  2017-En-30241  What if.... the Metro LRT went over the Walter...   \n",
       "4011  2017-En-22137  Somewhere between  #hope and #despair the day ...   \n",
       "4012  2017-En-30539  Decide to stop being afraid. To just stop. Wha...   \n",
       "4013  2017-En-11685  @robert7019 @itzamadhouse @skyler12388\\r\\ralso...   \n",
       "4014  2017-En-40354           @chelseafc let them know it's the #blues   \n",
       "4015  2017-En-20122  Well this is flipping great! Flipping standsti...   \n",
       "4016  2017-En-40378  the waitress recognised me from last time i wa...   \n",
       "4017  2017-En-30311  Watch this amazing live.ly broadcast by @evanh...   \n",
       "4018  2017-En-31280             Go one extra mile just see her smiling   \n",
       "4019  2017-En-10383  Yet we still have deaths, road rage, &amp; vio...   \n",
       "4020  2017-En-40289       Dylon felt dejected. He has a dejected aunt!   \n",
       "4021  2017-En-40146             With loyal heart is concerned. #somber   \n",
       "4022  2017-En-10119      @XemitSellsMagic add tracking but resent them   \n",
       "4023  2017-En-11153  Having one of those #angry days. I will have t...   \n",
       "4024  2017-En-11123  @daemondave @paulkrugman Hey stupid, that was ...   \n",
       "4025           7102                                           im happy   \n",
       "\n",
       "       Affect  Intensity  \n",
       "0     sadness          1  \n",
       "1     sadness          1  \n",
       "2       anger          3  \n",
       "3         joy          2  \n",
       "4         joy          1  \n",
       "5     sadness          1  \n",
       "6       anger          2  \n",
       "7        fear          3  \n",
       "8       anger          2  \n",
       "9        fear          3  \n",
       "10       fear          2  \n",
       "11        joy          3  \n",
       "12        joy          3  \n",
       "13    sadness          1  \n",
       "14      anger          2  \n",
       "15    sadness          2  \n",
       "16        joy          2  \n",
       "17      anger          2  \n",
       "18      anger          2  \n",
       "19      anger          3  \n",
       "20    sadness          3  \n",
       "21       fear          1  \n",
       "22    sadness          1  \n",
       "23        joy          2  \n",
       "24       fear          1  \n",
       "25        joy          3  \n",
       "26    sadness          2  \n",
       "27      anger          2  \n",
       "28    sadness          3  \n",
       "29       fear          3  \n",
       "...       ...        ...  \n",
       "3996  sadness          2  \n",
       "3997  sadness          2  \n",
       "3998    anger          2  \n",
       "3999    anger          3  \n",
       "4000     fear          3  \n",
       "4001  sadness          1  \n",
       "4002     fear          2  \n",
       "4003    anger          3  \n",
       "4004    anger          3  \n",
       "4005  sadness          3  \n",
       "4006  sadness          3  \n",
       "4007      joy          3  \n",
       "4008    anger          2  \n",
       "4009      joy          1  \n",
       "4010      joy          2  \n",
       "4011     fear          2  \n",
       "4012      joy          1  \n",
       "4013    anger          2  \n",
       "4014  sadness          1  \n",
       "4015     fear          2  \n",
       "4016  sadness          1  \n",
       "4017      joy          2  \n",
       "4018      joy          2  \n",
       "4019    anger          2  \n",
       "4020  sadness          2  \n",
       "4021  sadness          3  \n",
       "4022    anger          3  \n",
       "4023    anger          3  \n",
       "4024    anger          3  \n",
       "4025      joy          0  \n",
       "\n",
       "[4026 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "test = pd.read_csv(\"DS-Test.csv\",encoding='ISO-8859-1')\n",
    "train = pd.read_csv(\"DS-Training.csv\",encoding='ISO-8859-1')\n",
    "train = train[train.Intensity != 0]\n",
    "train=train.append({'ID' : '7102' , 'content' : 'im happy','Affect' : 'joy','Intensity' : 0},ignore_index=True)\n",
    "train=train.drop(columns=['ID', 'Intensity'])\n",
    "train=train.append(text_emotion)\n",
    "train1=pd.read_csv(\"DS-Training.csv\",encoding='ISO-8859-1')\n",
    "train1 = train1[train1.Intensity != 0]\n",
    "train1=train1.append({'ID' : '7102' , 'content' : 'im happy','Affect' : 'joy','Intensity' : 0},ignore_index=True)\n",
    "\n",
    "train1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function converts a text to a sequence of words.\n",
    "def review_wordlist(review, remove_stopwords=False):\n",
    "    # Removing html tags\n",
    "    review_text = BeautifulSoup(review).get_text()\n",
    "    # Removing non-letter.\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \",review_text)\n",
    "    # Converting to lower case and splitting\n",
    "    trains_content = review_text.lower().split()\n",
    "    # Optionally remove stopwords\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))     \n",
    "        trains_content = [w for w in trains_content if not w in stops]\n",
    "\n",
    "    \n",
    "    return(trains_content)\n",
    "\n",
    "\n",
    "# word2vec expects a list of lists.\n",
    "# Using punkt tokenizer for better splitting of a paragraph into sentences.\n",
    "\n",
    "import nltk.data\n",
    "#nltk.download('popular')\n",
    "\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "# This function splits a review into sentences\n",
    "def review_sentences(review, tokenizer, remove_stopwords=False):\n",
    "    # 1. Using nltk tokenizer\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    sentences = []\n",
    "    # 2. Loop for each sentence\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence)>0:\n",
    "            sentences.append(review_wordlist(raw_sentence,\\\n",
    "                                            remove_stopwords))\n",
    "\n",
    "    # This returns the list of lists\n",
    "    return sentences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        VivYau it all doom gloom only want hear lovely...\n",
      "1        The 2nd step beating anxiety or depression rea...\n",
      "2        Forgot eat dinner now Im furious with everythi...\n",
      "3                              Your future bright Remember\n",
      "4        Check out this film Robocoq 301 animated short...\n",
      "5        jccrocker CharlotteProtest do u wait 4 fact vi...\n",
      "6        russbully Ended up paying 75p for half tube sm...\n",
      "7        Tho we havent talked Jeff but news so sad shoc...\n",
      "8        luv seeing man with scowl on his face walking ...\n",
      "9        Just want Saturday be over but then again want...\n",
      "10       love mother but talking about recent horrific ...\n",
      "11       Will WHU be old bill free by time game with Ch...\n",
      "12       Today reached 1000 subscriber on YT goodday th...\n",
      "13       On bedrest since got out hospital U find unope...\n",
      "14                         Idubbbz LeafyIsHere am offended\n",
      "15       MissFushiGaming hashtag thing kid always tell ...\n",
      "16               Do what make successful happy now forever\n",
      "17       Hey all white people out there are offended wh...\n",
      "18       atheist_taco DrJillStein well cucks among her ...\n",
      "19       im so mad about power ranger im incensed im fu...\n",
      "20       Honestly dont know why Im so unhappy most time...\n",
      "21       JaySekulow what can we do 2 get realDonaldTrum...\n",
      "22                                    LazyBoiSam blue blue\n",
      "23       Watch this amazing lively broadcast by kana_bl...\n",
      "24       panic attack AND CALL YOURSELF A REAL FAN make...\n",
      "25       WildRoverTours Thank for follow it good websit...\n",
      "26       How do u grieve someone who legally wasnt pers...\n",
      "27       noris_prosk8r2 Maybe dont provoke him future i...\n",
      "28            When will week full Mondays end disheartened\n",
      "29       With only 7 month left until posse undergradua...\n",
      "                               ...                        \n",
      "39911    Juniesgurl WOOHOO My broom isnt up standardsth...\n",
      "39912    Hoopsiscrazy U think web design wld go down we...\n",
      "39913                                     happy mother day\n",
      "39914                       watching Bolt with brother mom\n",
      "39922                billzucker Thanks for making me laugh\n",
      "39926      httpbitlyX7D1c Spanish Grand Prix 2009 one hour\n",
      "39927    instantmessaging with two favourite conversati...\n",
      "39933                            Had great time last night\n",
      "39939    watermelon39 haha And Twitter Hard though isnt it\n",
      "39940                     HosamKamel Thanks for follow man\n",
      "39949                          dai_bach daps were best lol\n",
      "39950          DebbieFletcher haha i will remember that xx\n",
      "39952    sharlynnx ME TOO please come online hope youve...\n",
      "39958    JamesHancox LOL or maybe it tooth fairy take e...\n",
      "39961    Wow Up coffee hand already outside So peaceful...\n",
      "39963    THE VIDEO IS FINALLY DONE WOOOOOOOOOOOOOOOOOOO...\n",
      "39964    Watched Wolverine yesterday spur moment kinda ...\n",
      "39965                                     heading off fair\n",
      "39966                   sunset view SO beautiful from room\n",
      "39967     McMedia Very well thank How are more importantly\n",
      "39968    muffinwomanxo EH u dont like retro tisk tisk w...\n",
      "39972    acchanosaurus good luck chan gue kmrn bawa bac...\n",
      "39980    Sitting Gatwick going home for week cant wait ...\n",
      "39981               maynaseric good luck with your auction\n",
      "39985    McMedia husband golfing amp Toddler shall frol...\n",
      "39986          going watch boy striped pjs hope i dont cry\n",
      "39987    gave bike thorough wash degrease it grease it ...\n",
      "39988    had SUCH AMAZING time last night McFly were IN...\n",
      "39994                          Succesfully following Tayla\n",
      "39998    niariley WASSUP BEAUTIFUL FOLLOW ME PEEP OUT M...\n",
      "Name: content, Length: 15833, dtype: object\n",
      "Parsing sentences from training set\n"
     ]
    }
   ],
   "source": [
    "#Removing punctuation problems\n",
    "train[\"content\"] = train[\"content\"].str.replace('[^\\w\\s]','')\n",
    "train[\"content\"].head()\n",
    "\n",
    "#Removing common words\n",
    "freq = pd.Series(' '.join(train[\"content\"]).split()).value_counts()[:10]\n",
    "freq = list(freq.index)\n",
    "train[\"content\"] = train[\"content\"].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
    "train[\"content\"].head()\n",
    "\n",
    "#Removing rarely occuring words\n",
    "freq = pd.Series(' '.join(train[\"content\"]).split()).value_counts()[-10:]\n",
    "freq = list(freq.index)\n",
    "train[\"content\"] = train[\"content\"].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
    "train[\"content\"].head()\n",
    "\n",
    "#Lemmatizaton is the much better option, it add the words into the root words\n",
    "train[\"content\"] = train[\"content\"].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
    "print (train[\"content\"])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sentences = []\n",
    "print(\"Parsing sentences from training set\")\n",
    "for review in train[\"content\"]:\n",
    "    sentences += review_sentences(review, tokenizer)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       VivYau it all doom gloom only want hear lovely...\n",
      "1       The 2nd step beating anxiety or depression rea...\n",
      "2       Forgot eat dinner now Im furious with everythi...\n",
      "3                             Your future bright Remember\n",
      "4       Check out this film Robocoq 301 animated short...\n",
      "5       jccrocker CharlotteProtest do u wait 4 fact vi...\n",
      "6       russbully Ended up paying 75p for half tube sm...\n",
      "7       Tho we havent talked Jeff but news so sad shoc...\n",
      "8       luv seeing man with scowl on his face walking ...\n",
      "9       Just want Saturday be over but then again want...\n",
      "10      love mother but talking about recent horrific ...\n",
      "11      Will WHU be old bill free by time game with Ch...\n",
      "12      Today reached 1000 subscriber on YT goodday th...\n",
      "13      On bedrest since got out hospital U find unope...\n",
      "14                        Idubbbz LeafyIsHere am offended\n",
      "15      MissFushiGaming hashtag thing kid always tell ...\n",
      "16              Do what make successful happy now forever\n",
      "17      Hey all white people out there are offended wh...\n",
      "18      atheist_taco DrJillStein well cucks among her ...\n",
      "19      im so mad about power ranger im incensed im fu...\n",
      "20      Honestly dont know why Im so unhappy most time...\n",
      "21      JaySekulow what can we do 2 get realDonaldTrum...\n",
      "22                                   LazyBoiSam blue blue\n",
      "23      Watch this amazing lively broadcast by kana_bl...\n",
      "24      panic attack AND CALL YOURSELF A REAL FAN make...\n",
      "25      WildRoverTours Thank for follow it good websit...\n",
      "26      How do u grieve someone who legally wasnt pers...\n",
      "27      noris_prosk8r2 Maybe dont provoke him future i...\n",
      "28           When will week full Mondays end disheartened\n",
      "29      With only 7 month left until posse undergradua...\n",
      "                              ...                        \n",
      "3996    ChipotleTweets Gabe worst part cant get hot st...\n",
      "3997    HillaryClinton TheDemocrats Its ok 2 be disgus...\n",
      "3998             if we let that id be fuming poor keeping\n",
      "3999    All hell breaking loose Charlotte CharlottePro...\n",
      "4000    Gosh anxiety attack turning into Panic attack ...\n",
      "4001    PatBlanchfield so mean like Uber but for despa...\n",
      "4002              dont want speak front him nopanicattack\n",
      "4003    Get gym discover forgot put gym shoe back bad ...\n",
      "4004    The ppl on here defending cop a they continue ...\n",
      "4005    My Fibromyalgia ha been really bad lately whic...\n",
      "4006    oh btw after 6 month depressionfree time got r...\n",
      "4007    Matalan when lady Xmas dept answer your call a...\n",
      "4008    Candices pout gonna take someone eye out mate ...\n",
      "4009    gypsydragoness wrinkling up Well now look here...\n",
      "4010           What if Metro LRT went over Walterdale yeg\n",
      "4011    Somewhere between hope despair day after meeti...\n",
      "4012    Decide stop being afraid To just stop What are...\n",
      "4013    robert7019 itzamadhouse skyler12388rralso cele...\n",
      "4014                      chelseafc let them know it blue\n",
      "4015    Well this flipping great Flipping standstill o...\n",
      "4016    waitress recognised me from last time i wa the...\n",
      "4017    Watch this amazing lively broadcast by evanhuf...\n",
      "4018               Go one extra mile just see her smiling\n",
      "4019    Yet we still have death road rage amp violatio...\n",
      "4020              Dylon felt dejected He ha dejected aunt\n",
      "4021                    With loyal heart concerned somber\n",
      "4022         XemitSellsMagic add tracking but resent them\n",
      "4023    Having one those angry day will have stop watc...\n",
      "4024    daemondave paulkrugman Hey stupid that wa bad ...\n",
      "4025                                             im happy\n",
      "Name: content, Length: 4026, dtype: object\n",
      "Parsing sentences from training set\n"
     ]
    }
   ],
   "source": [
    "#for intensity\n",
    "#Removing punctuation problems\n",
    "train1[\"content\"] = train1[\"content\"].str.replace('[^\\w\\s]','')\n",
    "train1[\"content\"].head()\n",
    "\n",
    "#Removing common words\n",
    "freq1 = pd.Series(' '.join(train1[\"content\"]).split()).value_counts()[:10]\n",
    "freq1 = list(freq1.index)\n",
    "train1[\"content\"] = train1[\"content\"].apply(lambda x1: \" \".join(x1 for x1 in x1.split() if x1 not in freq1))\n",
    "train1[\"content\"].head()\n",
    "\n",
    "#Removing rarely occuring words\n",
    "freq1 = pd.Series(' '.join(train1[\"content\"]).split()).value_counts()[-10:]\n",
    "freq1 = list(freq1.index)\n",
    "train1[\"content\"] = train1[\"content\"].apply(lambda x1: \" \".join(x1 for x1 in x1.split() if x1 not in freq))\n",
    "train1[\"content\"].head()\n",
    "\n",
    "#Lemmatizaton is the much better option, it add the words into the root words\n",
    "train1[\"content\"] = train1[\"content\"].apply(lambda x1: \" \".join([Word(word).lemmatize() for word in x1.split()]))\n",
    "print (train1[\"content\"])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sentences1 = []\n",
    "print(\"Parsing sentences from training set\")\n",
    "for review in train1[\"content\"]:\n",
    "    sentences1 += review_sentences(review, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the built-in logging module for saving the state of the model\n",
    "import logging\n",
    "import gensim.downloader as api\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model....\n"
     ]
    }
   ],
   "source": [
    "# Creating the model and setting values for the various parameters\n",
    "num_features = 300  # Word vector dimensionality\n",
    "min_word_count = 40 # Minimum word count\n",
    "num_workers = 4     # Number of parallel threads\n",
    "context = 10        # Context window size\n",
    "downsampling = 1e-3 # (0.001) Downsample setting for frequent words\n",
    "\n",
    "# Initializing the train model\n",
    "from gensim.models import word2vec\n",
    "print(\"Training model....\")\n",
    "model = word2vec.Word2Vec(sentences,\\\n",
    "                          workers=num_workers,\\\n",
    "                          size=num_features,\\\n",
    "                          min_count=min_word_count,\\\n",
    "                          window=context,\n",
    "                          sample=downsampling)\n",
    "\n",
    "# To make the model memory efficient\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# Saving the model for later use. Can be loaded using Word2Vec.load()\n",
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model....\n"
     ]
    }
   ],
   "source": [
    "#for intensity\n",
    "# Creating the model and setting values for the various parameters\n",
    "num_features1 = 300  # Word vector dimensionality\n",
    "min_word_count = 40 # Minimum word count\n",
    "num_workers = 4     # Number of parallel threads\n",
    "context = 10        # Context window size\n",
    "downsampling = 1e-3 # (0.001) Downsample setting for frequent words\n",
    "\n",
    "# Initializing the train model\n",
    "from gensim.models import word2vec\n",
    "print(\"Training model....\")\n",
    "model1 = word2vec.Word2Vec(sentences,\\\n",
    "                          workers=num_workers,\\\n",
    "                          size=num_features,\\\n",
    "                          min_count=min_word_count,\\\n",
    "                          window=context,\n",
    "                          sample=downsampling)\n",
    "\n",
    "# To make the model memory efficient\n",
    "model1.init_sims(replace=True)\n",
    "\n",
    "# Saving the model for later use. Can be loaded using Word2Vec.load()\n",
    "model_name1 = \"300features_40minwords_10context\"\n",
    "model1.save(model_name1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Talha Saleem\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(636, 300)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.wv.syn0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to average all word vectors in a paragraph\n",
    "def featureVecMethod(words, model, num_features):\n",
    "    # Pre-initialising empty numpy array for speed\n",
    "    featureVec = np.zeros(num_features,dtype=\"float32\")\n",
    "    nwords = 0\n",
    "    \n",
    "    #Converting Index2Word which is a list to a set for better speed in the execution.\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    \n",
    "    for word in  words:\n",
    "        if word in index2word_set:\n",
    "            nwords = nwords + 1\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    \n",
    "    # Dividing the result by number of words to get average\n",
    "    featureVec = np.divide(featureVec, nwords)\n",
    "    return featureVec\n",
    "\n",
    "# Function for calculating the average feature vector\n",
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    counter = 0\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    for review in reviews:\n",
    "        # Printing a status message every 1000th review\n",
    "        if counter%1000 == 0:\n",
    "            print(\"Review %d of %d\"%(counter,len(reviews)))\n",
    "            \n",
    "        reviewFeatureVecs[counter] = featureVecMethod(review, model, num_features)\n",
    "        counter = counter+1\n",
    "        \n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 15833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Talha Saleem\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  del sys.path[0]\n",
      "C:\\Users\\Talha Saleem\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in true_divide\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 1000 of 15833\n",
      "Review 2000 of 15833\n",
      "Review 3000 of 15833\n",
      "Review 4000 of 15833\n",
      "Review 5000 of 15833\n",
      "Review 6000 of 15833\n",
      "Review 7000 of 15833\n",
      "Review 8000 of 15833\n",
      "Review 9000 of 15833\n",
      "Review 10000 of 15833\n",
      "Review 11000 of 15833\n",
      "Review 12000 of 15833\n",
      "Review 13000 of 15833\n",
      "Review 14000 of 15833\n",
      "Review 15000 of 15833\n"
     ]
    }
   ],
   "source": [
    "# Calculating average feature vector for training set\n",
    "clean_train_reviews = []\n",
    "for review in train['content']:\n",
    "    clean_train_reviews.append(review_wordlist(review, remove_stopwords=True))\n",
    "    \n",
    "trainDataVecs = getAvgFeatureVecs(clean_train_reviews, model, num_features)\n",
    "#print (trainDataVecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 4026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Talha Saleem\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  del sys.path[0]\n",
      "C:\\Users\\Talha Saleem\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in true_divide\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 1000 of 4026\n",
      "Review 2000 of 4026\n",
      "Review 3000 of 4026\n",
      "Review 4000 of 4026\n"
     ]
    }
   ],
   "source": [
    "# Calculating average feature vector for training set\n",
    "clean_train_reviews1 = []\n",
    "for review in train1['content']:\n",
    "    clean_train_reviews1.append(review_wordlist(review, remove_stopwords=True))\n",
    "    \n",
    "trainDataVecs1 = getAvgFeatureVecs(clean_train_reviews1, model1, num_features1)\n",
    "#print (trainDataVecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 4068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Talha Saleem\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  del sys.path[0]\n",
      "C:\\Users\\Talha Saleem\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in true_divide\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 1000 of 4068\n",
      "Review 2000 of 4068\n",
      "Review 3000 of 4068\n",
      "Review 4000 of 4068\n"
     ]
    }
   ],
   "source": [
    "# Calculating average feature vactors for test set     \n",
    "clean_test_reviews = []\n",
    "for review in test[\"Tweet\"]:\n",
    "    clean_test_reviews.append(review_wordlist(review,remove_stopwords=True))\n",
    "    \n",
    "testDataVecs = getAvgFeatureVecs(clean_test_reviews, model, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras_preprocessing.text.Tokenizer object at 0x000001D258D704A8>\n",
      "28\n"
     ]
    }
   ],
   "source": [
    "max_features = 2000\n",
    "tokenizer = Tokenizer(num_words=max_features, split=' ')\n",
    "print (tokenizer)\n",
    "tokenizer.fit_on_texts(train['content'].values)\n",
    "X = tokenizer.texts_to_sequences(train[\"content\"].values)\n",
    "X = pad_sequences(X)\n",
    "print (X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras_preprocessing.text.Tokenizer object at 0x000001D258D656A0>\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "max_features = 2000\n",
    "tokenizer1 = Tokenizer(num_words=max_features, split=' ')\n",
    "print (tokenizer1)\n",
    "tokenizer1.fit_on_texts(train1['content'].values)\n",
    "X1 = tokenizer1.texts_to_sequences(train1[\"content\"].values)\n",
    "X1 = pad_sequences(X1)\n",
    "print (X1.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0310 16:42:57.600806  2480 deprecation_wrapper.py:119] From C:\\Users\\Talha Saleem\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0310 16:42:59.189522  2480 deprecation_wrapper.py:119] From C:\\Users\\Talha Saleem\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0310 16:43:00.051930  2480 deprecation_wrapper.py:119] From C:\\Users\\Talha Saleem\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0310 16:43:00.964685  2480 deprecation_wrapper.py:119] From C:\\Users\\Talha Saleem\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0310 16:43:01.046686  2480 deprecation.py:506] From C:\\Users\\Talha Saleem\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0310 16:43:12.069062  2480 deprecation_wrapper.py:119] From C:\\Users\\Talha Saleem\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0310 16:43:12.218337  2480 deprecation_wrapper.py:119] From C:\\Users\\Talha Saleem\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 28, 128)           256000    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 28, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 196)               254800    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4)                 788       \n",
      "=================================================================\n",
      "Total params: 511,588\n",
      "Trainable params: 511,588\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 128\n",
    "lstm_out = 196\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embed_dim,input_length = X.shape[1]))\n",
    "model.add(SpatialDropout1D(0.4))\n",
    "model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(4,activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 27, 128)           256000    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_2 (Spatial (None, 27, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 196)               254800    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 788       \n",
      "=================================================================\n",
      "Total params: 511,588\n",
      "Trainable params: 511,588\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embed_dim1 = 128\n",
    "lstm_out1 = 196\n",
    "\n",
    "model1 = Sequential()\n",
    "model1.add(Embedding(max_features, embed_dim1,input_length = X1.shape[1]))\n",
    "model1.add(SpatialDropout1D(0.4))\n",
    "model1.add(LSTM(lstm_out1, dropout=0.2, recurrent_dropout=0.2))\n",
    "model1.add(Dense(4,activation='softmax'))\n",
    "model1.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "print(model1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = pd.get_dummies(train['Affect']).values #thissssssssssssssssssssssssssssssssssssssssssssssssss\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y1 = pd.get_dummies(train1['Intensity']).values #thissssssssssssssssssssssssssssssssssssssssssssssssss\n",
    "X1_train, X1_test, Y1_train, Y1_test = train_test_split(X1,Y1, test_size = 0.3, random_state = 42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0310 16:43:16.296323  2480 deprecation.py:323] From C:\\Users\\Talha Saleem\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      " - 142s - loss: 0.9977 - acc: 0.5647\n",
      "Epoch 2/4\n",
      " - 162s - loss: 0.6742 - acc: 0.7389\n",
      "Epoch 3/4\n",
      " - 105s - loss: 0.5711 - acc: 0.7781\n",
      "Epoch 4/4\n",
      " - 216s - loss: 0.5203 - acc: 0.8024\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d254ceaf28>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "model.fit(X_train, Y_train, epochs = 4, batch_size=batch_size, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      " - 27s - loss: 1.1491 - acc: 0.3378\n",
      "Epoch 2/3\n",
      " - 18s - loss: 1.0485 - acc: 0.4319\n",
      "Epoch 3/3\n",
      " - 20s - loss: 0.8873 - acc: 0.5994\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d25c63b5f8>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "model1.fit(X1_train, Y1_train, epochs = 3, batch_size=batch_size, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# save the model to disk\\nfilename_emotion = 'emotion_model.sav'\\npickle.dump(model, open(filename_emotion, 'wb'))\\nfilename_intensity = 'intensity_model.sav'\\npickle.dump(model, open(filename_intensity, 'wb'))\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# save the model to disk\n",
    "filename_emotion = 'emotion_model.sav'\n",
    "pickle.dump(model, open(filename_emotion, 'wb'))\n",
    "filename_intensity = 'intensity_model.sav'\n",
    "pickle.dump(model, open(filename_intensity, 'wb'))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# load the model from disk\\nloaded_model = pickle.load(open(filename_emotion, 'rb'))\\nresult = loaded_model.score(X_test, Y_test)\\nprint(result)\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# load the model from disk\n",
    "loaded_model = pickle.load(open(filename_emotion, 'rb'))\n",
    "result = loaded_model.score(X_test, Y_test)\n",
    "print(result)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3250, 28)\n",
      "score: 0.68\n",
      "acc: 0.73\n"
     ]
    }
   ],
   "source": [
    "validation_size = 1500\n",
    "\n",
    "X_validate = X_test[-validation_size:]\n",
    "Y_validate = Y_test[-validation_size:]\n",
    "X_test = X_test[:-validation_size]\n",
    "Y_test = Y_test[:-validation_size]\n",
    "print (X_test.shape)\n",
    "score,acc = model.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)\n",
    "print(\"score: %.2f\" % (score))\n",
    "print(\"acc: %.2f\" % (acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vector = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "twt: [[2, 53]]\n",
      "2\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "twt = [\"i am insulted\"]\n",
    "#vectorizing the tweet by the pre-fitted tokenizer instance\n",
    "twt = tokenizer.texts_to_sequences(twt)\n",
    "\n",
    "\n",
    "\n",
    "print (\"twt:\", twt)\n",
    "#padding the tweet to have exactly the same shape as `embedding_2` input\n",
    "twt = pad_sequences(twt, maxlen=28, dtype='int32', value=0)\n",
    "twt1 = pad_sequences(twt, maxlen=27, dtype='int32', value=0)\n",
    "    \n",
    "sentiment = model.predict(twt,batch_size=1,verbose = 2)[0]\n",
    "Intensity = model1.predict(twt1,batch_size=1,verbose = 2)[0]\n",
    "    \n",
    "    \n",
    "label=int(np.argmax(sentiment))\n",
    "intensity=int(np.argmax(Intensity))\n",
    "\n",
    "\n",
    "\n",
    "print (label)\n",
    "print (intensity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vector = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "socket binded to port 5020\n",
      "socket is listening\n",
      "Connected to : 192.168.100.18 : 48308\n",
      "{'Sentence': 'Im very sad today'}\n",
      "After tokenizing:  []\n"
     ]
    }
   ],
   "source": [
    "from _thread import *\n",
    "from socket import *\n",
    "import threading \n",
    "import socket\n",
    "import json\n",
    "from io import StringIO\n",
    "twt_add=[]\n",
    "def threaded(c):\n",
    "    data = c.recv(1024)\n",
    "    data = json.loads( data.decode(\"utf-8\")  )\n",
    "    print(data)\n",
    "\n",
    "    twt_add.append(str(data[\"Sentence\"]))\n",
    "    #print(\"twt: \",str(twt_add))\n",
    "    #vectorizing the tweet by the pre-fitted tokenizer instance\n",
    "    twt = tokenizer.texts_to_sequences(twt_add)\n",
    "    del twt_add[-1]\n",
    "    print (\"After tokenizing: \", twt_add)\n",
    "    \n",
    "    #padding the tweet to have exactly the same shape as `embedding_2` input\n",
    "    twt = pad_sequences(twt, maxlen=28, dtype='int32', value=0)\n",
    "    twt1 = pad_sequences(twt, maxlen=27, dtype='int32', value=0)\n",
    "\n",
    "    sentiment = model.predict(twt,batch_size=1,verbose = 2)\n",
    "    Intensity = model1.predict(twt1,batch_size=1,verbose = 2)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    #print(\"Labelsssss: \", sentiment )\n",
    "    #print (\"Intensityssss\",intensity)\n",
    "    \n",
    "    \n",
    "    label1=int(np.argmax(sentiment))\n",
    "    intensity1=int(np.argmax(Intensity))\n",
    "\n",
    "    \n",
    "    #print(\"Label1: \", label1 )\n",
    "    #print (\"Intensity1\",intensity1)\n",
    "    intensity1+=1\n",
    "    \n",
    "    #vector = count_vector.transform( [data[\"Sentence\"]] )\n",
    "    label_1 = str(label1)\n",
    "    intensity_1=str(intensity1)\n",
    "    #print(\"Label_1: \", label_1 )\n",
    "    #print (\"Intensity_1\",intensity_1)\n",
    "    concat= label_1+\",\"+intensity_1\n",
    "    c.send(  json.dumps({\"label\": (concat) }).encode('utf-8')   )\n",
    "    c.close()\n",
    "port = 5020\n",
    "s = socket.socket(socket.AF_INET, socket.SOCK_STREAM) \n",
    "s.setsockopt(SOL_SOCKET, SO_REUSEADDR, 1)\n",
    "s.bind((  \"0.0.0.0\" , port  )) \n",
    "print(\"socket binded to port\", port )\n",
    "s.listen(5) \n",
    "print(\"socket is listening\")\n",
    "all_conn = []\n",
    "while True:\n",
    "    conn, addr = s.accept()\n",
    "    print('Connected to :', addr[0], ':', addr[1])\n",
    "    all_conn.append( conn )\n",
    "    start_new_thread(threaded, (conn,))\n",
    "s.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
