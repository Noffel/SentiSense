{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n#Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport csv\nimport itertools\nimport sys\nfrom pandas import DataFrame\nfrom keras.models import Sequential\n\nfrom keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\nfrom keras.layers import Dropout\nfrom keras.layers import Activation\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport nltk\nfrom nltk.corpus import stopwords\nfrom textblob import TextBlob\nfrom nltk.stem import PorterStemmer\nfrom textblob import Word\n\n# set the matplotlib backend so figures can be saved in the background\nimport matplotlib\nmatplotlib.use(\"Agg\")\n \n# import the necessary packages\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.optimizers import Adam\nfrom keras.preprocessing.image import img_to_array\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.model_selection import train_test_split\n#from pyimagesearch.smallervggnet import SmallerVGGNet\nimport matplotlib.pyplot as plt\n%matplotlib inline\n#from imutils import paths\nimport argparse\nimport random\nimport pickle\nimport cv2\n\nfrom bs4 import BeautifulSoup \nimport re # For regular expressions\n\n\n# Fitting a random forest classifier to the training data\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.impute import SimpleImputer\n\n\n#For Tokenizing and pad sequence\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\n\n\nimport string\n\n#For Lemmatization\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk import word_tokenize","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\ntext_emotion = pd.read_csv(\"../input/text_emotion.csv\")\ntext_emotion=text_emotion[text_emotion.sentiment != 'enthusiasm']\ntext_emotion=text_emotion[text_emotion.sentiment != 'neutral']\ntext_emotion=text_emotion[text_emotion.sentiment != 'empty']\ntext_emotion=text_emotion[text_emotion.sentiment != 'surprise']\ntext_emotion=text_emotion[text_emotion.sentiment != 'boredom']\ntext_emotion=text_emotion[text_emotion.sentiment != 'relief']\ntext_emotion=text_emotion[text_emotion.sentiment != 'worry']\ntext_emotion=text_emotion[text_emotion.sentiment != 'love']\ntext_emotion=text_emotion[text_emotion.sentiment != 'fun']\ntext_emotion['sentiment'] = text_emotion['sentiment'].map({'sadness': \"sadness\",'hate':'anger',\n                                                           'happiness': \"joy\",\n                                                           'anger': \"anger\"})\ntext_emotion=text_emotion.drop(columns=['tweet_id', 'author'])\n\ntext_emotion = text_emotion.reindex(sorted(text_emotion.columns), axis=1)\ntext_emotion.columns = ['content', 'Affect']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\ntest = pd.read_csv(\"../input/DS-Test.csv\",encoding='ISO-8859-1')\ntrain = pd.read_csv(\"../input/DS-Training.csv\",encoding='ISO-8859-1')\ntrain = train[train.Intensity != '0']\ntrain = train[train.Intensity != 0]\ntrain=train.drop(columns=['ID','Intensity'])\ntrain.columns=['content','Affect']\ntrain=train.append(text_emotion)\ntrain1=pd.read_csv(\"../input/DS-Training.csv\",encoding='ISO-8859-1')\n\n#train1 = train1[train1.Intensity != '0']\ntrain1 = train1[train1.Intensity != 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainsad = pd.read_csv(\"../input/DS-Training.csv\",encoding='ISO-8859-1')\ntrainfear = pd.read_csv(\"../input/DS-Training.csv\",encoding='ISO-8859-1')\ntrainanger = pd.read_csv(\"../input/DS-Training.csv\",encoding='ISO-8859-1')\ntrainjoy = pd.read_csv(\"../input/DS-Training.csv\",encoding='ISO-8859-1')\ntrainsad=trainsad[trainsad.Affect == \"sadness\"]\ntrainjoy=trainjoy[trainjoy.Affect == \"joy\"]\ntrainfear=trainfear[trainfear.Affect == \"fear\"]\ntrainanger=trainanger[trainanger.Affect == \"anger\"]\n\n\n\ntrainsad=trainsad[trainsad.Intensity != 0]\ntrainjoy=trainjoy[trainjoy.Intensity != 0]\ntrainfear=trainfear[trainfear.Intensity != 0]\ntrainanger=trainanger[trainanger.Intensity != 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainfear","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This function converts a text to a sequence of words.\ndef review_wordlist(review, remove_stopwords=False):\n    # Removing html tags\n    review_text = BeautifulSoup(review).get_text()\n    # Removing non-letter.\n    review_text = re.sub(\"[^a-zA-Z]\",\" \",review_text)\n    # Converting to lower case and splitting\n    trains_content = review_text.lower().split()\n    # Optionally remove stopwords\n    if remove_stopwords:\n        stops = set(stopwords.words(\"english\"))     \n        trains_content = [w for w in trains_content if not w in stops]\n\n    \n    return(trains_content)\n\n\n# word2vec expects a list of lists.\n# Using punkt tokenizer for better splitting of a paragraph into sentences.\n\nimport nltk.data\n#nltk.download('popular')\n\ntokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n\n# This function splits a review into sentences\ndef review_sentences(review, tokenizer, remove_stopwords=False):\n    # 1. Using nltk tokenizer\n    raw_sentences = tokenizer.tokenize(review.strip())\n    sentences = []\n    # 2. Loop for each sentence\n    for raw_sentence in raw_sentences:\n        if len(raw_sentence)>0:\n            sentences.append(review_wordlist(raw_sentence,\\\n                                            remove_stopwords))\n\n    # This returns the list of lists\n    return sentences\n\n\n\n\n#Removing punctuation problems\ntrain[\"content\"] = train[\"content\"].str.replace('[^\\w\\s]','')\ntrain[\"content\"].head()\n\n#Removing common words\nfreq = pd.Series(' '.join(train[\"content\"]).split()).value_counts()[:10]\nfreq = list(freq.index)\ntrain[\"content\"] = train[\"content\"].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\ntrain[\"content\"].head()\n\n#Removing rarely occuring words\nfreq = pd.Series(' '.join(train[\"content\"]).split()).value_counts()[-10:]\nfreq = list(freq.index)\ntrain[\"content\"] = train[\"content\"].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\ntrain[\"content\"].head()\n\n#Lemmatizaton is the much better option, it add the words into the root words\ntrain[\"content\"] = train[\"content\"].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\nprint (train[\"content\"])\n\nsentences = []\nprint(\"Parsing sentences from training set\")\nfor review in train[\"content\"]:\n    sentences += review_sentences(review, tokenizer)\n    \n    \n    \n    \n    \n    \n#for intensity\n\n\n\n\n\n#Removing punctuation problems\ntrain1[\"content\"] = train1[\"content\"].str.replace('[^\\w\\s]','')\ntrain1[\"content\"].head()\n\n#Removing common words\nfreq1 = pd.Series(' '.join(train1[\"content\"]).split()).value_counts()[:10]\nfreq1 = list(freq1.index)\ntrain1[\"content\"] = train1[\"content\"].apply(lambda x1: \" \".join(x1 for x1 in x1.split() if x1 not in freq1))\ntrain1[\"content\"].head()\n\n#Removing rarely occuring words\nfreq1 = pd.Series(' '.join(train1[\"content\"]).split()).value_counts()[-10:]\nfreq1 = list(freq1.index)\ntrain1[\"content\"] = train1[\"content\"].apply(lambda x1: \" \".join(x1 for x1 in x1.split() if x1 not in freq))\ntrain1[\"content\"].head()\n\n#Lemmatizaton is the much better option, it add the words into the root words\ntrain1[\"content\"] = train1[\"content\"].apply(lambda x1: \" \".join([Word(word).lemmatize() for word in x1.split()]))\nprint (train1[\"content\"])\n\nsentences1 = []\nprint(\"Parsing sentences from training set\")\nfor review in train1[\"content\"]:\n    sentences1 += review_sentences(review, tokenizer)\n    \n    \n    \n    \n#sad\n\n\n\n#Removing punctuation problems\ntrainsad[\"content\"] = trainsad[\"content\"].str.replace('[^\\w\\s]','')\ntrainsad[\"content\"].head()\n\n#Removing common words\nfreq = pd.Series(' '.join(trainsad[\"content\"]).split()).value_counts()[:10]\nfreq = list(freq.index)\ntrainsad[\"content\"] = trainsad[\"content\"].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\ntrainsad[\"content\"].head()\n\n#Removing rarely occuring words\nfreq = pd.Series(' '.join(trainsad[\"content\"]).split()).value_counts()[-10:]\nfreq = list(freq.index)\ntrainsad[\"content\"] = trainsad[\"content\"].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\ntrainsad[\"content\"].head()\n\n#Lemmatizaton is the much better option, it add the words into the root words\ntrainsad[\"content\"] = trainsad[\"content\"].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\nprint (trainsad[\"content\"])\n\nsentences = []\nprint(\"Parsing sentences from training set\")\nfor review in trainsad[\"content\"]:\n    sentences += review_sentences(review, tokenizer)\n\n    \n    \n    \n#joy\n\n\n\n\n\n#Removing punctuation problems\ntrainjoy[\"content\"] = trainjoy[\"content\"].str.replace('[^\\w\\s]','')\ntrainjoy[\"content\"].head()\n\n#Removing common words\nfreq = pd.Series(' '.join(trainjoy[\"content\"]).split()).value_counts()[:10]\nfreq = list(freq.index)\ntrainjoy[\"content\"] = trainjoy[\"content\"].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\ntrainjoy[\"content\"].head()\n\n#Removing rarely occuring words\nfreq = pd.Series(' '.join(trainjoy[\"content\"]).split()).value_counts()[-10:]\nfreq = list(freq.index)\ntrainjoy[\"content\"] = trainjoy[\"content\"].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\ntrainjoy[\"content\"].head()\n\n#Lemmatizaton is the much better option, it add the words into the root words\ntrainjoy[\"content\"] = trainjoy[\"content\"].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\nprint (trainjoy[\"content\"])\n\nsentences = []\nprint(\"Parsing sentences from training set\")\nfor review in trainjoy[\"content\"]:\n    sentences += review_sentences(review, tokenizer)\n  \n    \n    \n#fear\n\n\n\n\n\n#Removing punctuation problems\ntrainfear[\"content\"] = trainfear[\"content\"].str.replace('[^\\w\\s]','')\ntrainfear[\"content\"].head()\n\n#Removing common words\nfreq = pd.Series(' '.join(trainfear[\"content\"]).split()).value_counts()[:10]\nfreq = list(freq.index)\ntrainfear[\"content\"] = trainfear[\"content\"].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\ntrainfear[\"content\"].head()\n\n#Removing rarely occuring words\nfreq = pd.Series(' '.join(trainfear[\"content\"]).split()).value_counts()[-10:]\nfreq = list(freq.index)\ntrainfear[\"content\"] = trainfear[\"content\"].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\ntrainfear[\"content\"].head()\n\n#Lemmatizaton is the much better option, it add the words into the root words\ntrainfear[\"content\"] = trainfear[\"content\"].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\nprint (trainfear[\"content\"])\n\nsentences = []\nprint(\"Parsing sentences from training set\")\nfor review in trainfear[\"content\"]:\n    sentences += review_sentences(review, tokenizer)\n  \n    \n    \n    \n#anger\n\n\n\n#Removing punctuation problems\ntrainanger[\"content\"] = trainanger[\"content\"].str.replace('[^\\w\\s]','')\ntrainanger[\"content\"].head()\n\n#Removing common words\nfreq = pd.Series(' '.join(trainanger[\"content\"]).split()).value_counts()[:10]\nfreq = list(freq.index)\ntrainanger[\"content\"] = trainanger[\"content\"].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\ntrainanger[\"content\"].head()\n\n#Removing rarely occuring words\nfreq = pd.Series(' '.join(trainanger[\"content\"]).split()).value_counts()[-10:]\nfreq = list(freq.index)\ntrainanger[\"content\"] = trainanger[\"content\"].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\ntrainanger[\"content\"].head()\n\n#Lemmatizaton is the much better option, it add the words into the root words\ntrainanger[\"content\"] = trainanger[\"content\"].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\nprint (trainanger[\"content\"])\n\nsentences = []\nprint(\"Parsing sentences from training set\")\nfor review in trainanger[\"content\"]:\n    sentences += review_sentences(review, tokenizer)\n  \n    \n    \n    \n    \n    \n    \n    \n    \n    \n# Creating the model and setting values for the various parameters\nnum_features = 300  # Word vector dimensionality\nmin_word_count = 40 # Minimum word count\nnum_workers = 4     # Number of parallel threads\ncontext = 10        # Context window size\ndownsampling = 1e-3 # (0.001) Downsample setting for frequent words\n\n# Initializing the train model\nfrom gensim.models import word2vec\nprint(\"Training model....\")\nmodel = word2vec.Word2Vec(sentences,\\\n                          workers=num_workers,\\\n                          size=num_features,\\\n                          min_count=min_word_count,\\\n                          window=context,\n                          sample=downsampling)\n\n# To make the model memory efficient\nmodel.init_sims(replace=True)\n\n# Saving the model for later use. Can be loaded using Word2Vec.load()\nmodel_name = \"300features_40minwords_10context\"\nmodel.save(model_name)\n\n\n\n#for intensity\n# Creating the model and setting values for the various parameters\nnum_features1 = 300  # Word vector dimensionality\nmin_word_count = 40 # Minimum word count\nnum_workers = 4     # Number of parallel threads\ncontext = 10        # Context window size\ndownsampling = 1e-3 # (0.001) Downsample setting for frequent words\n\n# Initializing the train model\nfrom gensim.models import word2vec\nprint(\"Training model....\")\nmodel1 = word2vec.Word2Vec(sentences,\\\n                          workers=num_workers,\\\n                          size=num_features,\\\n                          min_count=min_word_count,\\\n                          window=context,\n                          sample=downsampling)\n\n# To make the model memory efficient\nmodel1.init_sims(replace=True)\n\n# Saving the model for later use. Can be loaded using Word2Vec.load()\nmodel_name1 = \"300features_40minwords_10context\"\nmodel1.save(model_name1)\n\n\n# Function to average all word vectors in a paragraph\ndef featureVecMethod(words, model, num_features):\n    # Pre-initialising empty numpy array for speed\n    featureVec = np.zeros(num_features,dtype=\"float32\")\n    nwords = 0\n    \n    #Converting Index2Word which is a list to a set for better speed in the execution.\n    index2word_set = set(model.wv.index2word)\n    \n    for word in  words:\n        if word in index2word_set:\n            nwords = nwords + 1\n            featureVec = np.add(featureVec,model[word])\n    \n    # Dividing the result by number of words to get average\n    featureVec = np.divide(featureVec, nwords)\n    return featureVec\n\n# Function for calculating the average feature vector\ndef getAvgFeatureVecs(reviews, model, num_features):\n    counter = 0\n    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n    for review in reviews:\n        # Printing a status message every 1000th review\n        if counter%1000 == 0:\n            print(\"Review %d of %d\"%(counter,len(reviews)))\n            \n        reviewFeatureVecs[counter] = featureVecMethod(review, model, num_features)\n        counter = counter+1\n        \n    return reviewFeatureVecs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sad\n\nnum_features = 300  # Word vector dimensionality\nmin_word_count = 40 # Minimum word count\nnum_workers = 4     # Number of parallel threads\ncontext = 10        # Context window size\ndownsampling = 1e-3 # (0.001) Downsample setting for frequent words\n\n# Initializing the train model\nfrom gensim.models import word2vec\nprint(\"Training model....\")\nmodel2 = word2vec.Word2Vec(sentences,\\\n                          workers=num_workers,\\\n                          size=num_features,\\\n                          min_count=min_word_count,\\\n                          window=context,\n                          sample=downsampling)\n\n# To make the model memory efficient\nmodel2.init_sims(replace=True)\n\n# Saving the model for later use. Can be loaded using Word2Vec.load()\nmodel_name2 = \"300features_40minwords_10context\"\nmodel2.save(model_name2)\n\n\n\n\n#joy\n\nnum_features = 300  # Word vector dimensionality\nmin_word_count = 40 # Minimum word count\nnum_workers = 4     # Number of parallel threads\ncontext = 10        # Context window size\ndownsampling = 1e-3 # (0.001) Downsample setting for frequent words\n\n# Initializing the train model\nfrom gensim.models import word2vec\nprint(\"Training model....\")\nmodel3 = word2vec.Word2Vec(sentences,\\\n                          workers=num_workers,\\\n                          size=num_features,\\\n                          min_count=min_word_count,\\\n                          window=context,\n                          sample=downsampling)\n\n# To make the model memory efficient\nmodel3.init_sims(replace=True)\n\n# Saving the model for later use. Can be loaded using Word2Vec.load()\nmodel_name3 = \"300features_40minwords_10context\"\nmodel3.save(model_name3)\n\n\n#fear\n\n\nnum_features = 300  # Word vector dimensionality\nmin_word_count = 40 # Minimum word count\nnum_workers = 4     # Number of parallel threads\ncontext = 10        # Context window size\ndownsampling = 1e-3 # (0.001) Downsample setting for frequent words\n\n# Initializing the train model\nfrom gensim.models import word2vec\nprint(\"Training model....\")\nmodel4 = word2vec.Word2Vec(sentences,\\\n                          workers=num_workers,\\\n                          size=num_features,\\\n                          min_count=min_word_count,\\\n                          window=context,\n                          sample=downsampling)\n\n# To make the model memory efficient\nmodel4.init_sims(replace=True)\n\n# Saving the model for later use. Can be loaded using Word2Vec.load()\nmodel_name4 = \"300features_40minwords_10context\"\nmodel4.save(model_name4)\n\n\n\n#anger\n\n\n\nnum_features = 300  # Word vector dimensionality\nmin_word_count = 40 # Minimum word count\nnum_workers = 4     # Number of parallel threads\ncontext = 10        # Context window size\ndownsampling = 1e-3 # (0.001) Downsample setting for frequent words\n\n# Initializing the train model\nfrom gensim.models import word2vec\nprint(\"Training model....\")\nmodel5 = word2vec.Word2Vec(sentences,\\\n                          workers=num_workers,\\\n                          size=num_features,\\\n                          min_count=min_word_count,\\\n                          window=context,\n                          sample=downsampling)\n\n# To make the model memory efficient\nmodel5.init_sims(replace=True)\n\n# Saving the model for later use. Can be loaded using Word2Vec.load()\nmodel_name5 = \"300features_40minwords_10context\"\nmodel5.save(model_name5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculating average feature vector for training set\nclean_train_reviews = []\nfor review in train['content']:\n    clean_train_reviews.append(review_wordlist(review, remove_stopwords=True))\n    \ntrainDataVecs = getAvgFeatureVecs(clean_train_reviews, model, num_features)\n#print (trainDataVecs)\n\n# Calculating average feature vector for training set\nclean_train_reviews1 = []\nfor review in train1['content']:\n    clean_train_reviews1.append(review_wordlist(review, remove_stopwords=True))\n    \ntrainDataVecs1 = getAvgFeatureVecs(clean_train_reviews1, model1, num_features1)\n#print (trainDataVecs)\n\n\n# Calculating average feature vactors for test set     \nclean_test_reviews = []\nfor review in test[\"Tweet\"]:\n    clean_test_reviews.append(review_wordlist(review,remove_stopwords=True))\n    \ntestDataVecs = getAvgFeatureVecs(clean_test_reviews, model, num_features)\n\nmax_features = 2000\ntokenizer = Tokenizer(num_words=max_features, split=' ')\nprint (tokenizer)\ntokenizer.fit_on_texts(train['content'].values)\nX = tokenizer.texts_to_sequences(train[\"content\"].values)\nX = pad_sequences(X)\nprint (X.shape[1])\n\n\nmax_features = 2000\ntokenizer1 = Tokenizer(num_words=max_features, split=' ')\nprint (tokenizer1)\ntokenizer1.fit_on_texts(train1['content'].values)\nX1 = tokenizer1.texts_to_sequences(train1[\"content\"].values)\nX1 = pad_sequences(X1)\nprint (X1.shape[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_train_reviews2 = []\nfor review in trainsad['content']:\n    clean_train_reviews2.append(review_wordlist(review, remove_stopwords=True))\ntrainDataVecs2 = getAvgFeatureVecs(clean_train_reviews2, model2, num_features)\n\n\nclean_train_reviews3 = []\nfor review in trainjoy['content']:\n    clean_train_reviews3.append(review_wordlist(review, remove_stopwords=True))\ntrainDataVecs3 = getAvgFeatureVecs(clean_train_reviews3, model3, num_features)\n\nclean_train_reviews4 = []\nfor review in trainfear['content']:\n    clean_train_reviews4.append(review_wordlist(review, remove_stopwords=True))  \ntrainDataVecs4 = getAvgFeatureVecs(clean_train_reviews4, model4, num_features)\n\n\nclean_train_reviews5 = []\nfor review in trainanger['content']:\n    clean_train_reviews5.append(review_wordlist(review, remove_stopwords=True))\ntrainDataVecs5 = getAvgFeatureVecs(clean_train_reviews5, model5, num_features)\n\n\n\n\nmax_features = 2000\ntokenizer2 = Tokenizer(num_words=max_features, split=' ')\nprint (tokenizer2)\ntokenizer2.fit_on_texts(trainsad['content'].values)\nX2 = tokenizer2.texts_to_sequences(trainsad[\"content\"].values)\nX2 = pad_sequences(X2)\nprint (X2.shape[1])\n\n\n\n\nmax_features = 2000\ntokenizer3 = Tokenizer(num_words=max_features, split=' ')\nprint (tokenizer3)\ntokenizer3.fit_on_texts(trainjoy['content'].values)\nX3 = tokenizer3.texts_to_sequences(trainjoy[\"content\"].values)\nX3 = pad_sequences(X3)\nprint (X3.shape[1])\n\n\n\nmax_features = 2000\ntokenizer4 = Tokenizer(num_words=max_features, split=' ')\nprint (tokenizer4)\ntokenizer4.fit_on_texts(trainfear['content'].values)\nX4 = tokenizer4.texts_to_sequences(trainfear[\"content\"].values)\nX4 = pad_sequences(X4)\nprint (X4.shape[1])\n\n\n\nmax_features = 2000\ntokenizer5 = Tokenizer(num_words=max_features, split=' ')\nprint (tokenizer5)\ntokenizer5.fit_on_texts(trainanger['content'].values)\nX5 = tokenizer5.texts_to_sequences(trainanger[\"content\"].values)\nX5 = pad_sequences(X5)\nprint (X5.shape[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embed_dim = 128\nlstm_out = 256\nmodel = Sequential()\nmodel.add(Embedding(max_features, embed_dim,input_length = X.shape[1]))\nmodel.add(SpatialDropout1D(0.4))\nmodel.add(LSTM(lstm_out, dropout=0.3, recurrent_dropout=0.2))\nmodel.add(Dense(4,activation='softmax'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\nprint(model.summary())\n\n\nembed_dim1 = 128\nlstm_out1 = 256\nmodel1 = Sequential()\nmodel1.add(Embedding(max_features, embed_dim1,input_length = X1.shape[1]))\nmodel1.add(SpatialDropout1D(0.4))\nmodel1.add(LSTM(lstm_out1, dropout=0.3, recurrent_dropout=0.3))\nmodel1.add(Dense(4,activation='softmax'))\nmodel1.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\nprint(model1.summary())\n\n\n\n\nembed_dim2 = 128\nlstm_out2 = 256\nmodel2 = Sequential()\nmodel2.add(Embedding(max_features, embed_dim2,input_length = X2.shape[1]))\nmodel2.add(SpatialDropout1D(0.4))\nmodel2.add(LSTM(lstm_out2, dropout=0.3, recurrent_dropout=0.3))\nmodel2.add(Dense(3,activation='softmax'))\nmodel2.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\nprint(model2.summary())\n\n\n\n\nembed_dim3 = 128\nlstm_out3 = 256\nmodel3 = Sequential()\nmodel3.add(Embedding(max_features, embed_dim3,input_length = X3.shape[1]))\nmodel3.add(SpatialDropout1D(0.4))\nmodel3.add(LSTM(lstm_out3, dropout=0.3, recurrent_dropout=0.3))\nmodel3.add(Dense(3,activation='softmax'))\nmodel3.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\nprint(model3.summary())\n\n\n\nembed_dim4 = 128\nlstm_out4 = 256\nmodel4 = Sequential()\nmodel4.add(Embedding(max_features, embed_dim4,input_length = X4.shape[1]))\nmodel4.add(SpatialDropout1D(0.4))\nmodel4.add(LSTM(lstm_out4, dropout=0.3, recurrent_dropout=0.3))\nmodel4.add(Dense(3,activation='softmax'))\nmodel4.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\nprint(model4.summary())\n\n\n\n\n\n\nembed_dim5 = 128\nlstm_out5 = 256\nmodel5 = Sequential()\nmodel5.add(Embedding(max_features, embed_dim5,input_length = X5.shape[1]))\nmodel5.add(SpatialDropout1D(0.4))\nmodel5.add(LSTM(lstm_out5, dropout=0.3, recurrent_dropout=0.3))\nmodel5.add(Dense(3,activation='softmax'))\nmodel5.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\nprint(model5.summary())\n\n\n\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y = pd.get_dummies(train['Affect']).values #this\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.3, random_state = 42)\n\nY1 = pd.get_dummies(train1['Intensity']).values #this\nX1_train, X1_test, Y1_train, Y1_test = train_test_split(X1,Y1, test_size = 0.3, random_state = 42)\n\nY2 = pd.get_dummies(trainsad['Intensity']).values #this\nX2_train, X2_test, Y2_train, Y2_test = train_test_split(X2,Y2, test_size = 0.3, random_state = 42)\n\nY3 = pd.get_dummies(trainjoy['Intensity']).values #this\nX3_train, X3_test, Y3_train, Y3_test = train_test_split(X3,Y3, test_size = 0.3, random_state = 42)\n\n\nY4 = pd.get_dummies(trainfear['Intensity']).values #this\nX4_train, X4_test, Y4_train, Y4_test = train_test_split(X4,Y4, test_size = 0.3, random_state = 42)\n\nY5 = pd.get_dummies(trainanger['Intensity']).values #this\nX5_train, X5_test, Y5_train, Y5_test = train_test_split(X5,Y5, test_size = 0.3, random_state = 42)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 32\nmodel2.fit(X2_train, Y2_train, epochs = 150, batch_size=batch_size, verbose = 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 32\nmodel3.fit(X3_train, Y3_train, epochs = 150, batch_size=batch_size, verbose = 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 32\nmodel4.fit(X4_train, Y4_train, epochs = 150, batch_size=batch_size, verbose = 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 32\nmodel5.fit(X5_train, Y5_train, epochs = 150, batch_size=batch_size, verbose = 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"psad=model2.predict(X2_test)\nb = np.zeros_like(psad)\nb[np.arange(len(psad)), psad.argmax(1)] = 1\nb=b.astype(int)\nc = np.zeros_like(Y2_test)\nc[np.arange(len(Y2_test)), Y2_test.argmax(1)] = 1\nc=c.astype(int)\nfrom sklearn.metrics import classification_report,confusion_matrix\nprint(classification_report(c,b))\n\n\npjoy=model3.predict(X3_test)\nb = np.zeros_like(pjoy)\nb[np.arange(len(pjoy)), pjoy.argmax(1)] = 1\nb=b.astype(int)\nc = np.zeros_like(Y3_test)\nc[np.arange(len(Y3_test)), Y3_test.argmax(1)] = 1\nc=c.astype(int)\nfrom sklearn.metrics import classification_report,confusion_matrix\nprint(classification_report(c,b))\n\n\n\npfear=model4.predict(X4_test)\nb = np.zeros_like(pfear)\nb[np.arange(len(pfear)), pfear.argmax(1)] = 1\nb=b.astype(int)\nc = np.zeros_like(Y4_test)\nc[np.arange(len(Y4_test)), Y4_test.argmax(1)] = 1\nc=c.astype(int)\nfrom sklearn.metrics import classification_report,confusion_matrix\nprint(classification_report(c,b))\n\n\n\npanger=model5.predict(X5_test)\nb = np.zeros_like(panger)\nb[np.arange(len(panger)), panger.argmax(1)] = 1\nb=b.astype(int)\nc = np.zeros_like(Y5_test)\nc[np.arange(len(Y5_test)), Y5_test.argmax(1)] = 1\nc=c.astype(int)\nfrom sklearn.metrics import classification_report,confusion_matrix\nprint(classification_report(c,b))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Decision tree classifier on both emotion and intenisty\n\n\nfrom sklearn.tree import tree\ndtree = tree.DecisionTreeClassifier()\ndtree2 = tree.DecisionTreeClassifier()\ndtree.fit(X_train, Y_train)\ndtree2.fit(X1_train, Y1_train)\npredictions = dtree.predict(X_test)\npredictions2 = dtree2.predict(X1_test)\nfrom sklearn.metrics import classification_report,confusion_matrix\nprint (\"F1 Score of emotion classification from Decision Tree Classifier\")\nprint (\" \")\nprint(classification_report(Y_test,predictions))\nprint (\" \")\nprint (\"F1 Score of intensity classification from Decision Tree Classifier\")\nprint (\" \")\nprint(classification_report(Y1_test,predictions2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import tree\ndtree3 = tree.DecisionTreeClassifier()\ndtree3.fit(X2_train, Y2_train)\np = dtree3.predict(X2_test)\nfrom sklearn.metrics import classification_report,confusion_matrix\nprint (\"F1 Score of emotion classification from Decision Tree Classifier\")\nprint (\" \")\nprint(classification_report(Y2_test,p))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Decision Tree Regressor for intensity\nprint (\"Decision tree regressor F1 score for intenisty\")\nprint (\"\")\nfrom sklearn.tree import DecisionTreeRegressor  \nregressor = DecisionTreeRegressor(random_state = 0)  \nregressor.fit(X1_train, Y1_train) \npredictions = regressor.predict(X1_test)\npredictions = predictions.astype(int)\nprint(classification_report(Y1_test,predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Rndom Forest classifier for both emotion and intensity\nfrom sklearn.ensemble import RandomForestClassifier\nrforest = RandomForestClassifier()\nrforest.fit(X_train, Y_train)\nrforest1 = RandomForestClassifier()\nrforest1.fit(X1_train, Y1_train)\np = rforest.predict(X_test)\nprint (\"F1 score for Emotion Classification from Random Forest Classifier\")\nprint (\"\")\nprint(classification_report(Y_test,p))\np1 = rforest1.predict(X1_test)\nprint (\"F1 score for Intensity Classification from Random Forest Classifier\")\nprint (\"\")\nprint(classification_report(Y1_test,p1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Random Forest Regressor for intenisty\nfrom sklearn.ensemble import RandomForestRegressor\nrfregressor = RandomForestRegressor(n_estimators = 200\n                                    , random_state = 1) \nrfregressor.fit(X1_train, Y1_train) \nrfpredictions = rfregressor.predict(X1_test)\nb = np.zeros_like(rfpredictions)\nb[np.arange(len(rfpredictions)), rfpredictions.argmax(1)] = 1\nb=b.astype(int)\nrfpredictions=b\n#rfpredictions = rfpredictions.astype(int)\nprint (\" F1 score for intenisty using random forest regressor\")\nprint (\"\")\nprint(classification_report(Y1_test,rfpredictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#naive bayes \n\nY=[]\nx=0\nl=len(Y1_train)\nval=0\nwhile x<l:\n    val=np.where(Y1_train[x] == 1)\n    Y.append(val[0][0])\n    x+=1\nfrom sklearn.naive_bayes import GaussianNB\nmnb = GaussianNB()\nmnb.fit(X1_train, Y)\ny_pred = mnb.predict(X1_test)\nY1=[]\nx=0\nl=len(Y1_test)\nval=0\nwhile x<l:\n    val=np.where(Y1_test[x] == 1)\n    Y1.append(val[0][0])\n    x+=1\n    from sklearn import metrics\n\n# Model Accuracy, how often is the classifier correct?\nprint(\"Accuracy:\",metrics.accuracy_score(Y1, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#naive bayes emotion\n\nY=[]\nx=0\nl=len(Y_train)\nval=0\nwhile x<l:\n    val=np.where(Y_train[x] == 1)\n    Y.append(val[0][0])\n    x+=1\nfrom sklearn.naive_bayes import GaussianNB\nmnb = GaussianNB()\nmnb.fit(X_train, Y)\ny_pred = mnb.predict(X_test)\nY1=[]\nx=0\nl=len(Y_test)\nval=0\nwhile x<l:\n    val=np.where(Y_test[x] == 1)\n    Y1.append(val[0][0])\n    x+=1\n    from sklearn import metrics\n\n# Model Accuracy, how often is the classifier correct?\nprint(\"Accuracy:\",metrics.accuracy_score(Y1, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#linear Regression \n\nfrom sklearn import datasets, linear_model, metrics \nreg = linear_model.LinearRegression() \n\nreg.fit(X1_train, Y1_train) \np=reg.predict(X1_test)\nb = np.zeros_like(p)\nb[np.arange(len(p)), p.argmax(1)] = 1\nb=b.astype(int)\np=b\nprint(classification_report(Y1_test,p))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#SVM \n\nY=[]\nx=0\nl=len(Y1_train)\nval=0\nwhile x<l:\n    val=np.where(Y1_train[x] == 1)\n    Y.append(val[0][0])\n    x+=1\nfrom sklearn import svm\nclf = svm.SVC(max_iter=1000000)\nclf.fit(X1_train, Y)\np1=clf.predict(X1_test)\nY1=[]\nx=0\nl=len(Y1_test)\nval=0\nwhile x<l:\n    val=np.where(Y1_test[x] == 1)\n    Y1.append(val[0][0])\n    x+=1\nprint(classification_report(Y1,p1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nY=[]\nx=0\nl=len(Y_train)\nval=0\nwhile x<l:\n    val=np.where(Y_train[x] == 1)\n    Y.append(val[0][0])\n    x+=1\nfrom sklearn import svm\nclf = svm.SVC(decision_function_shape='ovo')\nclf.fit(X_train, Y)\np1=clf.predict(X_test)\nY1=[]\nx=0\nl=len(Y_test)\nval=0\nwhile x<l:\n    val=np.where(Y_test[x] == 1)\n    Y1.append(val[0][0])\n    x+=1\nprint(classification_report(Y1,p1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Logistic Regression\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix,accuracy_score\nY=[]\nx=0\nl=len(Y_train)\nval=0\nwhile x<l:\n    val=np.where(Y_train[x] == 1)\n    Y.append(val[0][0])\n    x+=1\nlogmodel = LogisticRegression()\nlogmodel.fit(X_train, Y)\np=logmodel.predict(X_test)\nY1=[]\nx=0\nl=len(Y_test)\nval=0\nwhile x<l:\n    val=np.where(Y_test[x] == 1)\n    Y1.append(val[0][0])\n    x+=1\nprint(classification_report(Y1,p))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import datasets, linear_model, metrics \nreg = linear_model.LinearRegression() \n\nreg.fit(X_train, Y_train) \np=reg.predict(X_test)\nb = np.zeros_like(p)\nb[np.arange(len(p)), p.argmax(1)] = 1\nb=b.astype(int)\np=b\nprint(classification_report(Y_test,p))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#emotion\n\nobjects = ('Neural \\n Networks', 'Random \\n Forests', 'Decision \\n Trees','SVM', 'Logistic \\n Regression')\ny_pos = np.arange(len(objects))\nperformance = [0.74,0.4,0.5,0.4,0.41]\n\n\nplt.bar(y_pos, performance,align='center', alpha=1)\nplt.xticks(y_pos, objects)\nplt.ylabel('Accuracy Scores')\nplt.title('Emotion Classification accuracies from different models')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Intensity\n\nobjects = ('Neural \\n Networks', 'Random \\n Forests', 'Decision \\n Trees','SVM', 'Logistic \\n Regression','Naive \\n Bayes')\ny_pos = np.arange(len(objects))\nperformance = [0.53,0.36,0.37,0.36,0.36,0.37]\n\n\nplt.bar(y_pos, performance,align='center', alpha=1)\nplt.xticks(y_pos, objects)\nplt.ylabel('Accuracy Scores')\nplt.title('Intensity Classification accuracies from different models')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}
